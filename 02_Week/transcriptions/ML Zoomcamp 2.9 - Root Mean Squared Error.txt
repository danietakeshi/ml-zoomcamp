[0:0 --> 0:00:04.74]	  Welcome back. This is lesson nine of the session two of machine learning Zoom camp.
[0:00:05.08 --> 0:00:10.64]	  And we will talk about evaluating regression models. And we will talk about one way of doing this,
[0:00:11.04 --> 0:00:15.70]	  which is root mean square error. And in the previous session, we trained our first model.
[0:0 --> 0:00:21.40]	  It was a baseline model that used only numerical features. And then we plotted the predictions,
[0:00:21.62 --> 0:00:29.22]	  we plotted the actual values. And we saw that they are a bit off, but we don't have a way to
[0:00:30.36 --> 0:00:35.56]	  quantify how bad the model is. And this is what we will do in this lesson.
[0:00:36.44 --> 0:00:44.70]	  So let me start with writing a formula first. And then we will decompose the formula and we'll try
[0:00:44.70 --> 0:00:54.46]	  to make sense of this. So root mean squared error. First of all, this is the difference between so
[0:00:54.86 --> 0:01:02.18]	  for each prediction that we have. So this G X I, this is the prediction we make for
[0:01:02.18 --> 0:01:05.22]	  X I. So this prediction
[0:01:05.22 --> 0:01:16.08]	  for X I. And then we have the actual value. So this is the actual value.
[0:01:22.18 --> 0:01:27.68]	  So this is the actual value for the observation I. So what we do is we take
[0:01:27.68 --> 0:01:34.16]	  the difference between them and then square this difference. So this is actually this squared error
[0:01:34.16 --> 0:01:42.28]	  part. And then what we do is we take the average of these differences. So for that,
[0:01:42.84 --> 0:01:49.12]	  we have a sum. A sum goes from 1 to M, where M is the number of different observations.
[0:01:49.78 --> 0:01:56.78]	  And then we take an average of it and we take a square root. So now let's decompose it.
[0:01:57.50 --> 0:02:05.20]	  So this part here is the square difference. So let me just write it here.
[0:02:09.02 --> 0:02:16.98]	  So this is the difference between the prediction and the actual value for like the actual price.
[0:02:17.34 --> 0:02:27.34]	  So this is price, price and this prediction. So imagine we have an array with predictions.
[0:02:29.12 --> 0:02:36.34]	  So this is our Y underscore pret. And we have some predictions here now. 10,
[0:02:36.60 --> 0:02:48.96]	  9, 11 and so on. So these are our predictions. Could be like 10. And then we have the actual price.
[0:02:55.22 --> 0:03:06.92]	  So this is our Y train. And then the price could be 9 here, 9 here, maybe 10.5 here.
[0:03:07.90 --> 0:03:17.32]	  And then perhaps 11.5 here. So these are predictions and these are the actual values.
[0:03:17.32 --> 0:03:21.50]	  So what we do here is we...
[0:03:21.50 --> 0:03:36.30]	  So what we do now here is we take the difference, which gives us another array with
[0:03:36.30 --> 0:03:45.34]	  differences. So here the difference is 1, here the difference is 0, here the difference is 0.5,
[0:03:45.34 --> 0:03:55.44]	  and here the difference is minus 1.5. Then what we do next is we square this difference.
[0:03:56.10 --> 0:04:09.86]	  Take this difference and square it. And for that, so for the square of 1 is 1, square of 0 is 0.
[0:04:09.92 --> 0:04:22.58]	  The square for 0.5 is 0.25. And the square of 1.5, 2.25. So we have that. So what we do next is
[0:04:22.58 --> 0:04:35.82]	  let me just write it one more time. So we have 1, we have 0, we have 0.25, 2.25. So this is our
[0:04:36.24 --> 0:04:44.56]	  square error. And now what we need to do is we need to take an average. So for us, the average is
[0:04:44.56 --> 0:05:03.74]	  1 plus 0, plus 0.25, plus 2.25 divided by 4. So we have 0.875. So this is our mean square error.
[0:05:09.06 --> 0:05:15.16]	  And then finally what we need to do is we take this one and we take this compute the square root of
[0:05:15.16 --> 0:05:26.06]	  this, which is 0.93. So for this case, so if we have, if our predictions look like this
[0:05:26.06 --> 0:05:32.28]	  and actual prices look like that, then for just these four observations, this will be
[0:05:32.28 --> 0:05:39.54]	  root mean square error. Yeah, let's implement this. Let's implement this. So let me just
[0:05:39.54 --> 0:05:46.22]	  write the formula here one more time. So first of all, it's again predictions,
[0:05:47.30 --> 0:05:54.44]	  the square difference between the predictions and the actual value. Then we have the mean here.
[0:05:54.88 --> 0:06:01.40]	  And then we have the square root. Yeah, let's call it RMSE. So then we have two values here.
[0:06:01.40 --> 0:06:07.74]	  So first is y, the actual values. And then we have y-preth, which are the predictions.
[0:06:08.28 --> 0:06:16.38]	  So here we need to compute this error, the difference between y and y-preth. Then square
[0:06:16.38 --> 0:06:26.22]	  error would be this error squared. Right? And then once the error is squared, we can compute the mean.
[0:06:26.58 --> 0:06:30.94]	  Actually, I mean, so we don't need to compute the sum and then divide it by the number of elements.
[0:06:31.42 --> 0:06:37.80]	  In AmPy, we can directly invoke the mean method. And this will give us
[0:06:37.80 --> 0:06:44.66]	  mean square error. Right? And then to compute the root mean square error, what we need to do is
[0:06:44.66 --> 0:06:51.32]	  compute the square error. Right? And this is basically what we return in results.
[0:06:53.80 --> 0:07:02.58]	  And let's use it. So we can use it for this. So we already have some predictions.
[0:07:04.18 --> 0:07:11.06]	  And we can use it for evaluating the quality of this. And we can actually simplify this formula
[0:07:11.06 --> 0:07:16.50]	  a little bit. So we don't need to have this error thing here. We can immediately square them.
[0:07:18.56 --> 0:07:25.62]	  So maybe this looks a bit nicer. So we implemented root mean square error. And now
[0:07:25.62 --> 0:07:29.34]	  let's use it on our validation set in the next lesson.
