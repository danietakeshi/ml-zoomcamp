[0:00:00.68 --> 0:00:05.34]	  Welcome back. This is lesson six of session two of machine learning Zoom
[0:00:05.34 --> 0:00:10.92]	  come and we already talked about a linear regression model and we saw how it
[0:00:10.92 --> 0:00:16.36]	  works on one simple example and now we'll try to generalize it to
[0:00:16.36 --> 0:00:24.84]	  a vector form. So how to talk about this and look at all the matrices we have.
[0:00:24.84 --> 0:00:34.18]	  So remember we talked about this capital X here. So how do we go from this
[0:00:34.18 --> 0:00:37.58]	  notation to this one? So let's talk about this.
[0:00:39.96 --> 0:00:49.22]	  So first of all let's again write it for a single vector. So remember we have the
[0:00:49.22 --> 0:00:59.26]	  bias term and then we have a sum that goes from 1 to n then we multiply the feature
[0:00:59.26 --> 0:01:07.70]	  with its weight and this is the formal. So this is how we make a prediction
[0:01:08.90 --> 0:01:20.86]	  for a specific car. And if you look at this you can see that this is nothing else but a dot product.
[0:01:21.14 --> 0:01:26.62]	  So we talked about dot products. This is vector-vector multiplication and we can actually
[0:01:26.62 --> 0:01:35.74]	  rewrite it slightly differently using this vector multiplication notation. So this will be vector
[0:01:37.14 --> 0:01:46.68]	  xe we transpose it and we multiply it with just the weight vector.
[0:01:48.50 --> 0:01:55.92]	  So like this notation is a bit more compact so we can implement this. So we already have this
[0:01:55.92 --> 0:02:03.76]	  linear regression function. Let's adjust it. So first we will create a function that is doing
[0:02:03.76 --> 0:02:13.50]	  the dot product and if you remember we already implemented this. We called it vector-vector
[0:02:13.50 --> 0:0]	  multiplication. So we start with result which is zero and then for each element we multiply them
[0:02:24.68 --> 0:02:31.18]	  with each other. So xig times
[0:02:31.18 --> 0:02:42.08]	  wg here it's of course sum and we return the result. So this is the product. So we don't need
[0:02:42.08 --> 0:02:48.14]	  here anything anymore. So now what we need to do is the result will be just
[0:02:48.20 --> 0:02:54.04]	  middle zero plus dot product between xi and w.
[0:02:55.86 --> 0:03:04.46]	  So yeah using this function our notation simplifies quite a bit. So our code simplifies quite a bit
[0:03:04.46 --> 0:03:07.28]	  and yeah so now we implemented this.
[0:03:09.32 --> 0:03:16.92]	  And as we discussed actually like this dot like if we use numpy we can just use numpy's
[0:03:19.92 --> 0:03:26.88]	  dot. Okay we can make it even shorter. Let's look at this. So now we have this
[0:03:26.88 --> 0:03:36.70]	  v this bias term that is kind of hanging out there all alone without x. So it's kind of
[0:03:36.70 --> 0:03:45.36]	  we can try to bring it in in this dot product. For that we can see we can imagine that there is a
[0:03:46.92 --> 0:04:00.06]	  there is one extra feature in our car that is let's call it i0 i1 we'll call it x i0 and this
[0:04:00.06 --> 0:04:07.64]	  feature is always one. So if it's one then it means that so let me just move it a little bit.
[0:04:09.92 --> 0:04:11.94]	  Let's this one also.
[0:04:14.44 --> 0:04:19.42]	  And so if it's one it means that it doesn't really
[0:04:19.42 --> 0:04:25.06]	  affect it so we can imagine that it doesn't exist so it's one always.
[0:04:26.26 --> 0:04:42.94]	  So then our vector w becomes w0, w1, w2 and so on up to wn. So this is a n plus one dimensional vector.
[0:04:43.32 --> 0:04:53.70]	  So we have n elements here and then one extra one. And then this x vector the features it becomes
[0:04:54.30 --> 0:05:10.68]	  this x i0 x i1 x i2 and so on x in. And here this this is just one. So and when we
[0:05:10.68 --> 0:05:19.24]	  and when we do dot product between them so when we do this
[0:05:20.40 --> 0:05:23.36]	  or the other way around we do
[0:05:23.36 --> 0:05:34.02]	  that matter. So we multiply w0 with one which ends leaves us with zero and then we basically
[0:05:34.02 --> 0:05:41.08]	  have the dot product between the rest like previously. So which is equivalent to what we have.
[0:05:41.92 --> 0:05:47.76]	  And it means that what we can do is just use the dot product notation for the entire
[0:05:48.18 --> 0:05:57.98]	  for the entire linear regression. So let's let's do this. So we can call it w new which will be
[0:05:57.98 --> 0:06:04.56]	  our so we don't need to do it inside.
[0:06:05.14 --> 0:06:17.58]	  So this will be our new vector with weights and this actually this way like if you do plus on
[0:06:17.58 --> 0:0]	  Python this it creates a new list where it just prepends it at the beginning. So this is exactly
[0:0 --> 0:06:32.26]	  what we need. And now this w new contains the bias term and then the rest of the features.
[0:06:33.10 --> 0:06:43.02]	  And then we do the same with x i we'll just prepent i at the beginning or prepent one at the
[0:06:43.02 --> 0:06:52.06]	  beginning. And now we do a dot product between this and this w new and the result should be the same.
[0:06:52.66 --> 0:06:57.52]	  So we see that the result is the same.
[0:06:59.74 --> 0:07:07.30]	  Okay so now we have this in vector term in vector form. So now let's go back to
[0:07:07.98 --> 0:07:17.16]	  thinking about all the examples not just one. And in this case we have this matrix which is capital X
[0:07:17.16 --> 0:07:27.42]	  number. And then in this matrix we have ones always at the beginning. So this is because we have
[0:07:27.42 --> 0:07:36.46]	  one here all the time. And then we have for each row we have the the features. So let's say
[0:07:38.04 --> 0:07:44.10]	  it's the first row. So it's first number one, row number two, three and four.
[0:07:45.32 --> 0:07:56.10]	  So these are the features of the first row. These are the features of the second row and so on.
[0:07:56.66 --> 0:08:06.40]	  Then we have for the last one we can even call it perhaps n. So for the last one it's or m because
[0:08:06.40 --> 0:08:21.24]	  we already have n here. So it's m and we have x m n. So this is this x for us is m by n plus one.
[0:08:21.52 --> 0:08:32.98]	  So there are m rows and n plus one columns right in this. And then we have vector w. These are
[0:08:33.42 --> 0:08:41.04]	  the weights. So we have zero and w o one and w n.
[0:08:41.94 --> 0:08:53.18]	  And what we need to do is for each of these rows, so for each of these rows we multiply a row with
[0:08:53.18 --> 0:08:58.68]	  this vector and then we do the same with this one and we do the same with this one.
[0:08:59.78 --> 0:09:11.08]	  So the result we have is for the first one it's let's call it x one, x two, x three, x m. So we have
[0:09:11.08 --> 0:09:22.04]	  x one dot product with w and we have the same for second and then we have the same for the last one.
[0:09:26.46 --> 0:09:28.90]	  And this is our predictions.
[0:09:31.14 --> 0:09:39.06]	  And we do this if we do this for all the rows, all the cars we have in the data set will be our
[0:09:39.06 --> 0:09:44.64]	  vector y predictions. So this is what we predict.
[0:09:46.34 --> 0:09:53.06]	  And you probably recognize by now that this looks very similar to
[0:09:53.06 --> 0:10:00.18]	  matrix multiplication. In fact, it is matrix multiplication. So what we need to do to apply
[0:10:00.18 --> 0:10:11.64]	  linear regression is we take this matrix x, we take this vector w and we just multiply them.
[0:10:12.36 --> 0:10:20.94]	  And this is our model. So what we need to do is just perform matrix vector multiplication.
[0:10:22.86 --> 0:10:32.48]	  And I think we can implement this. So just for as an example, I'll take this.
[0:10:35.30 --> 0:10:42.70]	  So let's say we have more, more different observations. So we have an observation that
[0:10:42.70 --> 0:10:50.32]	  you remember we need to add once. So let's say this one has 148 horsepower
[0:10:50.88 --> 0:10:55.80]	  than 24 miles per gallon. And then it has popularity of
[0:10:55.80 --> 0:11:09.64]	  1300. Then let's take another x and another one. Just make it aligned a bit. So this one,
[0:11:09.76 --> 0:11:18.88]	  let's say it has 132 horse horses. And then this one has 300 something. So this is actually the
[0:11:18.88 --> 0:11:29.14]	  same that let me just copy it here. So it's not very popular. It's not very popular, but
[0:11:29.14 --> 0:11:38.72]	  it's very powerful. So this has 25. And let's say it's quite popular. So we have these observations.
[0:11:39.46 --> 0:11:50.94]	  And now we can put them into a matrix. So for that, we just put them so x becomes a list of lists.
[0:11:51.40 --> 0:11:56.48]	  You can take a look at this. So it's a list of lists. And if we put this into NumPy,
[0:11:58.24 --> 0:12:05.34]	  so now it turns this list of lists into a matrix, two dimensional numbering.
[0:12:07.28 --> 0:12:18.48]	  So we have it. And we have our WNU vector, which I'll just
[0:12:18.48 --> 0:12:29.80]	  here. And that what we need to do now is just do the multiplication between this matrix and this
[0:12:32.24 --> 0:12:41.30]	  vector. And here we have predictions. So now for each car, we have three cars. Now for each car,
[0:12:41.42 --> 0:12:48.34]	  we have a prediction. What is the price for this car? And yeah, so this is how we can actually
[0:12:48.34 --> 0:12:55.66]	  implement a linear regression. So it will be something like a linear regression.
[0:12:57.86 --> 0:13:06.16]	  And so we have that. And we do vector matrix multiplication.
[0:13:07.82 --> 0:13:19.10]	  Linear regression. We have the results. Okay, so now we know how to do this for multiple rows at
[0:13:19.10 --> 0:13:25.52]	  the same time. So we first generalize it and went from just writing a for loop into recognizing
[0:13:25.52 --> 0:13:32.76]	  that this is a dot product between a vector with features and a vector with weights. Then we noticed
[0:13:32.76 --> 0:13:40.80]	  that we can make it even shorter by adding a fictional feature one, like a virtual feature
[0:13:40.80 --> 0:13:47.86]	  to our feature vector. So our became even shorter. And then we generalize it to
[0:13:49.74 --> 0:13:55.32]	  many examples to a complete feature matrix. And we saw that it's actually nothing else but
[0:13:55.32 --> 0:14:04.08]	  matrix vector multiplication. And you've been wondering like where these W's come from? Like
[0:14:04.08 --> 0:14:10.60]	  how do we set the values for them? And this is something that we'll talk about in the next lesson.
