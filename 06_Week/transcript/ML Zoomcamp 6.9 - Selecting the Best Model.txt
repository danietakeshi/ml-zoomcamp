[0:0 --> 0:00:05.36]	  Hello everyone, this is lesson 9 of machine learning ZoomCamp session 6 and in this lesson
[0:00:05.36 --> 0:00:11.86]	  we will train, we will select the final model. So far what we did is we trained three different
[0:00:11.86 --> 0:00:19.44]	  tree based models. First we trained the decision tree model, so that was our model, so we trained
[0:00:19.44 --> 0:00:26.26]	  it, so this is the parameters we selected, we thought are the best ones and this is how this
[0:00:26.26 --> 0:00:34.84]	  tree actually looks like, so let me copy this and again, so this is our best decision tree model,
[0:00:35.04 --> 0:00:41.06]	  then we did something similar with random first, I think this is our best random first,
[0:00:41.62 --> 0:00:44.74]	  here we wanted to leave it at 200,
[0:00:46.54 --> 0:00:52.28]	  put it here, this is the best random first model,
[0:00:57.90 --> 0:0]	  let me take that,
[0:01:01.34 --> 0:01:10.44]	  so this is our best random first model and then finally we have our best exebus model
[0:01:10.44 --> 0:01:19.06]	  that we also trained here, so what we want to do now is we want to relate these three models
[0:01:19.06 --> 0:01:24.84]	  on the validation data set and select the best one, we probably already know what is the best
[0:01:24.84 --> 0:01:31.44]	  model, we already saw it, let me do this anyways, so first we will evaluate the predictions of our
[0:01:31.44 --> 0:01:50.30]	  decision tree, so this is the performance of our decision tree, 78.5% AUC, then for random forest
[0:01:50.86 --> 0:02:00.82]	  I'll just copy paste this and replace dt with rf, so it is 4% better than decision tree and then
[0:02:00.82 --> 0:02:10.22]	  let's do the final one with random forest, here we do model predict and we use d validation
[0:02:10.22 --> 0:02:19.46]	  and now let's compute its rc and this is 1% better than random forest, so now what we want to do
[0:02:19.66 --> 0:02:26.24]	  is we see that exebus is the best model, now we want to train the final model and we want to use
[0:02:26.24 --> 0:02:33.10]	  the entire training data set, so we want to use this data frame full train, now we want to use it for
[0:02:33.10 --> 0:02:38.90]	  training our final model and then we want to evaluate this final model on our test data set,
[0:02:39.28 --> 0:02:46.64]	  so you see here we have the status, so we need to, so first let's reset the index, I don't like that
[0:02:46.64 --> 0:02:59.28]	  it's not sequential, then what we need to do next is we need to get our y, full train, so it's our
[0:02:59.88 --> 0:03:14.50]	  status, here it is text, so status default, let's type in values, so our full train
[0:03:14.50 --> 0:03:23.16]	  and then we need to delete it, delete the status column from our data frame, full train,
[0:03:23.98 --> 0:03:30.70]	  so we did that, now we need our dictionaries, dictionaries, full train,
[0:03:31.88 --> 0:03:40.36]	  so two dig variant records, now we need to turn these dictionaries into factors, we use
[0:03:40.36 --> 0:03:47.60]	  dictionary vectorizer for that, dictionary vectorizer, sparse, false and then we get our
[0:03:47.60 --> 0:03:56.36]	  full train, full train feature matrix here by doing fit transform of these dictionaries,
[0:03:57.20 --> 0:04:04.94]	  then I think we already have our y test, yes we do, so we only need to get the
[0:04:04.94 --> 0:04:14.90]	  feature matrix for our test data set, so let's get our dictionaries test, so here we have data frame
[0:04:14.90 --> 0:04:22.74]	  test, here x test which we transform our dictionaries,
[0:04:24.56 --> 0:04:31.70]	  let's run that, we have our feature matrices and now we need to create this d matrix
[0:04:31.70 --> 0:04:43.12]	  with this full feature matrix, then label will be by a full train and then feature names will be our
[0:04:43.44 --> 0:04:50.30]	  dictionary vectorizer, get feature names, so this is not d test but this is default train
[0:04:50.30 --> 0:04:59.22]	  and likewise we need also to do this for test, x test, actually we don't need labels here
[0:04:59.22 --> 0:05:05.40]	  because we will not use this for testing, we will evaluate it outside of easyboost, we will use
[0:05:05.40 --> 0:05:11.80]	  second learn for evaluation, so we don't need label here, we remove it, okay now let's get these
[0:05:12.46 --> 0:05:18.72]	  parameters and train our final model using full train, so these are the parameters and let's train it,
[0:05:19.26 --> 0:05:27.80]	  we have our final model and now let's evaluate it, model predict d test, we have our predictions,
[0:05:28.48 --> 0:05:34.74]	  this is how the predictions look like, let's take a look at the first 10, this is how our
[0:05:34.74 --> 0:05:47.08]	  prediction looks like, now let's compute the arucscore, wide test and we see that our arucscore is 83%
[0:05:47.08 --> 0:05:55.10]	  which is quite in line with what we had previously, so it's a little bit worse by
[0:05:55.10 --> 0:06:02.22]	  like a fraction of percent but it's fine, so it's not a big deal, so here we conclude that our model
[0:06:02.22 --> 0:06:08.98]	  didn't overfit, it did quite well, so it generalized quite well to unseen data and even though we
[0:06:08.98 --> 0:06:14.58]	  extensively used our validation set for tuning the parameters, we didn't accidentally overfit,
[0:06:14.84 --> 0:06:20.24]	  so the performance on the test data set is reasonably good, okay and this is our final model,
[0:06:20.50 --> 0:06:26.56]	  turned out that xgboost has the best performance, usually it's not a surprise to me when I hear
[0:06:26.56 --> 0:06:34.02]	  this, xgboost often turns out to be one of the best models, at least for tabular data, so for
[0:06:34.02 --> 0:06:40.08]	  tabular data meaning when what we have is a data frame with features, for this kind of datasets,
[0:06:40.36 --> 0:06:47.26]	  usually xgboost tends to have better performance than any other models, but the downside of this is
[0:06:47.26 --> 0:06:53.80]	  it's more complex, it's more difficult to tune, it has a lot more parameters, while let's say for
[0:06:53.80 --> 0:06:58.92]	  random forest it's just too important parameter set we need to tune, so it's a lot easier I would say
[0:06:58.92 --> 0:07:06.04]	  and for xgboost it's more parameters and it's easier to overfit with xgboost but can get better
[0:07:06.04 --> 0:07:13.88]	  performance with this, okay so we compared three models, we then concluded that xgboost was the
[0:07:13.88 --> 0:07:19.60]	  best one, we trained the final model and we concluded that the performance of this model is
[0:07:19.60 --> 0:07:26.10]	  good enough on the test dataset and what we will do next in the next lesson, we will go through
[0:07:26.10 --> 0:07:31.38]	  all the materials we covered here and we will summarize this session, so see you soon.
